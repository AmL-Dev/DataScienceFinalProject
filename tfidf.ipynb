{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF.IDF\n",
    "\n",
    "Term Frequency - Inverse Document Frequency (TF.IDF) is a technique used to identify the significance of each term (word) in a document, within a corpus (a collection of documents). \n",
    "\n",
    "For each book, we compute the tf.idf score of each word in the description. The top n most relevant words, i.e. the words with the highest tf.idf score, are selected as features of the book, and put in the item profile. \n",
    "\n",
    "The term frequency (TF) is given by\n",
    "\n",
    "$TF = \\frac{n}{N}$\n",
    "\n",
    "where n is the number of times a word appears in the document, and N is the total amount of words in the document.\n",
    "\n",
    "The inverse document frequency is given by\n",
    "\n",
    "$IDF = log_{2}(\\frac{M}{m})$\n",
    "\n",
    "where m is the number of documents that contain the word, and M is the total amount of documents in the corpus.\n",
    "\n",
    "The TF.IDF score is then calculated by multiplying TF and IDF scores:\n",
    "\n",
    "$TF.IDF = TF \\cdot IDF$\n",
    "\n",
    "### 4 map-reduce steps in order to calculate the weigths (i.e. tfidf score) of each term:\n",
    "\n",
    "#### 1. Calculate the number of times a word appears in the document (n)\n",
    "\n",
    "map: (id, content) &rarr; [(word,id), 1]\n",
    "<br>\n",
    "<br>\n",
    "reduce: ((word,id), [1]) &rarr; ((word, id), n)\n",
    "\n",
    "#### 2. Calculate the number of words in the document (N)\n",
    "\n",
    "map: ((word,id), n) &rarr; (id, (word, n))\n",
    "<br>\n",
    "<br>\n",
    "reduce: (id, [word, n]) &rarr; ((word, id), (n, N))\n",
    "\n",
    "#### 3. Calculate the number of documents that contain the word (m)\n",
    "\n",
    "map: ((word,id), (n,N)) &rarr; (word, (id, n, N, 1))\n",
    "<br>\n",
    "<br>\n",
    "reduce: (word, [id, n, N, 1]) &rarr; [(word, id), (n, N, m)]\n",
    "\n",
    "#### 3. Calculate the TF.IDF score for each (id,word)-pair\n",
    "\n",
    "map: ((word,id), (n, N, m)) &rarr; ((word, id), tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tfidf.py\n"
     ]
    }
   ],
   "source": [
    "%%file tfidf.py\n",
    " \n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import pandas as pd #using pandas to find the number of books in the corpus\n",
    "import spacy #using the spacy library to remove stop words \n",
    "from nltk.stem import PorterStemmer #reduces the word to the root node\n",
    "from nltk.stem import SnowballStemmer #using snowball to reduce the word to the root node \n",
    "import string \n",
    "import math\n",
    "import re \n",
    "\n",
    "#finding the total number of books in the corpus (needed for the tfidf calculations)\n",
    "test = pd.read_csv('data/books_testset.csv')\n",
    "M = len(test)\n",
    "\n",
    "#used to filter out stopwords from the description (english language small model of spacy)\n",
    "en = spacy.load('en_core_web_sm')\n",
    "stopwords = en.Defaults.stop_words\n",
    "\n",
    "#string of special characters, used to remove special characters from the description\n",
    "special_char = r'[' + string.punctuation + ']'\n",
    "\n",
    "#standardize words that share the same suffix and that normally are derivations of gramatically similar words\n",
    "#two different stemmers, the snowball is more agressive\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(language='english')\n",
    "\n",
    "#used to search for groups that have alphanumerics or ' that are 1 or longer. I.e. breaks a line into words.\n",
    "WORD_RE = re.compile(r\"[\\w']+\") \n",
    "\n",
    "#only yield the words with a tfidf score above the threshold, in order to reduce the number of computations\n",
    "threshold = 0.00000001\n",
    "\n",
    "\n",
    "class MR_TFIDF(MRJob): #class that inherits from MRJob \n",
    "\n",
    "\n",
    "    def steps(self):\n",
    "        #need to define the order of mappers and reducers, so that the functions takes in the right key and value\n",
    "        #only one mapper and reducer per step, so therefore need several steps\n",
    "        #do we need the combiner between the mapper and reducer??\n",
    "        return [ \n",
    "            MRStep(mapper=self.mapper_1, reducer=self.reducer_1),\n",
    "            MRStep(mapper=self.mapper_2, reducer = self.reducer_2),\n",
    "            MRStep(mapper = self.mapper_3, reducer = self.reducer_3),\n",
    "            MRStep(mapper = self.mapper_4)\n",
    "        ]\n",
    "\n",
    "    def split_and_clean(self, line):  #takes in a key and a value, here the key is ignored\n",
    "        words = []\n",
    "        line = re.sub(special_char, ' ', line) #remove special characters from line\n",
    "        for word in WORD_RE.findall(line): #for each word in that line\n",
    "            if word.lower() not in stopwords and len(word)>1: #if the word is not a stop word \n",
    "                words.append(snowball.stem(word.lower())) #append the root of the word\n",
    "        return words #return the words\n",
    "\n",
    "    #key - ignored\n",
    "    #values - corpus of documents (i.e. the data frame books_metadata)\n",
    "    def mapper_1(self, _, line):  #each line is a row, i.e. a book \n",
    "        idFound = re.search(r\"(^\\d+),\", line)\n",
    "        descriptionFound = re.search(r\",\\\"(.+)\\\",\", line)\n",
    "        if idFound: \n",
    "            id = int(idFound.group(1))\n",
    "            if descriptionFound:\n",
    "                d = descriptionFound.group(1)\n",
    "                description = str()\n",
    "                flag = True\n",
    "                for char in d:\n",
    "                    if char =='\"':\n",
    "                        flag = False\n",
    "                    if flag:\n",
    "                        description += char\n",
    "                for word in self.split_and_clean(description): \n",
    "                    yield (word,id), 1   # returns key-value pairs ((word, id), 1)\n",
    "\n",
    "\n",
    "    #key - the key which was yielded by the mapper, i.e. (word, id). (key[0] = word, key[1]=id)\n",
    "    #values - A generator which yields all values yielded by the mapper which correspond to key, i.e. a list of 1's\n",
    "    def reducer_1(self, word_id, counts): \n",
    "        yield (word_id[0], word_id[1]), sum(counts) # returns key-value pairs ((word, id), n)\n",
    "\n",
    "    #key - (word, id)\n",
    "    #values - n\n",
    "    def mapper_2(self, word_id, n):\n",
    "        word, id = word_id[0], word_id[1], \n",
    "        yield id, (word,n) # returns key-value pairs (id, (word,n))\n",
    "\n",
    "    #key - id\n",
    "    #values - list of (word,n) pairs \n",
    "    def reducer_2(self, id, word_n):\n",
    "        \n",
    "        word_list = []\n",
    "        n_list = []\n",
    "        N = 0 \n",
    "        num_el = 0\n",
    "\n",
    "        for value in word_n:\n",
    "            word, n = value[0], value[1]\n",
    "\n",
    "            word_list.append(word)\n",
    "            n_list.append(n)\n",
    "            N+=int(n)\n",
    "\n",
    "            num_el+=1\n",
    "\n",
    "        for i in range(num_el):\n",
    "            yield (word_list[i],id), (n_list[i],N)    #returns key-value pairs ((word,id), (n,N))\n",
    "\n",
    "    #key - (word, id)\n",
    "    #values - (n, N) (only one pair)\n",
    "    def mapper_3(self, word_id, n_N):\n",
    "        word, id = word_id[0], word_id[1]\n",
    "        n, N = n_N[0], n_N[1]\n",
    "        yield word, (id, n, N, 1)\n",
    "\n",
    "\n",
    "    #key - word\n",
    "    #values - list of key-value (id,n,N,1) pairs\n",
    "    def reducer_3(self, word, id_n_N_counts):\n",
    "\n",
    "        id_list = []\n",
    "        n_list = []\n",
    "        N_list = []\n",
    "        m = 0\n",
    "\n",
    "        num_el = 0\n",
    "\n",
    "        for value in id_n_N_counts:\n",
    "            id, n, N, count = value[0], value[1], value[2], value[3]\n",
    "\n",
    "            id_list.append(id)\n",
    "            n_list.append(n)\n",
    "            N_list.append(N)\n",
    "\n",
    "            m+=count\n",
    "\n",
    "            num_el +=1\n",
    "\n",
    "        for i in range(num_el):\n",
    "            yield (word, id_list[i]), (n_list[i], N_list[i],m)\n",
    "\n",
    "    def mapper_4(self, word_id, n_N_m):\n",
    "        word, id = word_id[0], word_id[1]\n",
    "        n, N, m = n_N_m[0], n_N_m[1], n_N_m[2]\n",
    "\n",
    "        tf = (n/N) \n",
    "        idf = math.log(M/m) \n",
    "        tfidf = tf*idf\n",
    "\n",
    "        if tfidf > threshold:\n",
    "            yield (word, id), tfidf\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MR_TFIDF.run()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/tfidf.sara.20221124.123756.535400\n",
      "Running step 1 of 4...\n",
      "Running step 2 of 4...\n",
      "Running step 3 of 4...\n",
      "Running step 4 of 4...\n",
      "job output is in /tmp/tfidf.sara.20221124.123756.535400/output\n",
      "Streaming final output from /tmp/tfidf.sara.20221124.123756.535400/output...\n",
      "Removing temp directory /tmp/tfidf.sara.20221124.123756.535400...\n"
     ]
    }
   ],
   "source": [
    "! python3 tfidf.py data/books_testset.csv > output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "filename = 'output'\n",
    "infile = open(filename, 'r')\n",
    "\n",
    "d = {}\n",
    "\n",
    "for line in infile:\n",
    "    wordFound = re.search(r'\\[\\\"(.+)\\\",', line)\n",
    "    idFound = re.search(r'\\[\\\".+\\\",(\\d+)\\]', line)\n",
    "    tfidfFound = re.search(r'\\]\\s+(\\d+\\.\\d+)', line)\n",
    "    if wordFound and idFound and tfidfFound:\n",
    "        word = wordFound.group(1)\n",
    "        id = int(idFound.group(1))\n",
    "        tfidf = float(tfidfFound.group(1))\n",
    "\n",
    "        if id in d.keys():\n",
    "            d[id].append((word, tfidf))\n",
    "        else:\n",
    "            d[id] = [(word, tfidf)]\n",
    "\n",
    "    \n",
    "infile.close()\n",
    "\n",
    "sorted_dict = {}\n",
    "n = 5 \n",
    "\n",
    "for key, values in d.items():\n",
    "    sorted_dict[key] = sorted(values, key=lambda value: value[1])[:5] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aae75fb4c4e28f3fda60e5eda70c237ec76be20ea0bc0d2e1a8874fab74ff8f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
