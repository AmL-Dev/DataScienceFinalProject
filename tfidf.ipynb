{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "Term Frequency - Inverse Document Frequency (TF-IDF) is a technique used to identify the significance of each term (word) in a document, within a corpus (a collection of documents). \n",
    "\n",
    "For each book, we compute the tfidf score of each word in the description. Later, only the k most relevant words, i.e. the k words with the highest tfidf score, are selected as features of the book, and put in the item profile. \n",
    "\n",
    "The term frequency (TF) is given by\n",
    "\n",
    "$TF = \\frac{n}{N}$\n",
    "\n",
    "where n is the number of times a word appears in the description, and N is the total amount of words in the description.\n",
    "\n",
    "The inverse document frequency (IDF) is given by\n",
    "\n",
    "$IDF = log(\\frac{M}{m})$\n",
    "\n",
    "where m is the number of description that contain the word, and M is the total number of descriptions (i.e. books) in the corpus.\n",
    "\n",
    "The TF-IDF score is then calculated by multiplying TF and IDF scores:\n",
    "\n",
    "$TFIDF = TF \\cdot IDF$\n",
    "\n",
    "### 4 MapReduce steps in order to calculate the weigths (i.e. tfidf score) of each term:\n",
    "\n",
    "#### 1. \n",
    "The first step calculates the number of times a word appears in the description (n). The mapper takes the book id (id) as input key and the book description (content) as input value, and outputs a key-value pair for every word in the description (i.e. word-id pairs). The reducer adds the values (1s) for each word-id pair, i.e. counts the number of times each word appears in the description.\n",
    "\n",
    "map: (id, content) &rarr; [(word,id), 1]\n",
    "<br>\n",
    "<br>\n",
    "reduce: ((word,id), [1]) &rarr; ((word, id), n)\n",
    "\n",
    "#### 2. \n",
    "The second step calculates the total amount of words in the description (N). The mapper takes in the word-id pair as input key and the number of times that word appears in the description (n) as input value, and rearranges the data by moving the word from key to value. The reducer takes the book id as input key, and finds the number of words in the description by adding all the associated n-values.\n",
    " \n",
    "map: ((word,id), n) &rarr; (id, (word, n))\n",
    "<br>\n",
    "<br>\n",
    "reduce: (id, [word, n]) &rarr; ((word, id), (n, N))\n",
    "\n",
    "#### 3. \n",
    "The third step calculates the number of descriptions that contain the word (m). The mapper rearranges the data by moving the id from key to value, and for each output pair it appends 1 to the value. In this way, the reducer is able to count the number of descriptions that contain the word (m). It takes the word as input key, and adds all the 1s associated with the word.\n",
    "\n",
    "map: ((word,id), (n,N)) &rarr; (word, (id, n, N, 1))\n",
    "<br>\n",
    "<br>\n",
    "reduce: (word, [id, n, N, 1]) &rarr; [(word, id), (n, N, m)]\n",
    "\n",
    "#### 4. \n",
    "The last step only consists of a mapper that calculates the tfidf-score for each id-word pair. The total number of books in the corpus (M), which is needed for these calculations, is found by converting the dataset into a pandas dataframe. The number of books is equal to the length of the dataframe, and provided as an input variable to this MapReduce job. \n",
    "\n",
    "map: ((word,id), (n, N, m)) &rarr; ((word, id), tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tfidf.py\n"
     ]
    }
   ],
   "source": [
    "%%file tfidf.py\n",
    "\n",
    "# Importing libraries\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import pandas as pd \n",
    "import spacy  \n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.stem import SnowballStemmer \n",
    "import string \n",
    "import math\n",
    "import re \n",
    "\n",
    "# Finding the total number of books in the corpus (needed for the tfidf calculations)\n",
    "test = pd.read_csv('data/books_metadata.csv')\n",
    "M = len(test)\n",
    "\n",
    "# Using the spacy library to load the english stopwords \n",
    "en = spacy.load('en_core_web_sm')\n",
    "stopwords = en.Defaults.stop_words\n",
    "\n",
    "# Using the punctuation attribute from the string library to store a string with all the special characters \n",
    "special_char = r'[' + string.punctuation + ']'\n",
    "\n",
    "# Using the stemmer to reduce the words to a common base form, by removing the affix\n",
    "# Two different stemmers, the snowball is more agressive\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(language='english')\n",
    "\n",
    "# Using regex to search for groups that have alphanumerics or ', i.e. to break a line into words\n",
    "WORD_RE = re.compile(r\"[\\w']+\") \n",
    "\n",
    "# To increase computation efficiency, MapReduce should only yield words with tfidf score above a specific threshold \n",
    "# An appropriate threshold is found calculationg the 5th procentile of the tfidf scores \n",
    "threshold = 0.0219\n",
    "\n",
    "\n",
    "class MR_TFIDF(MRJob):  \n",
    "\n",
    "    # 4 MapReduce steps \n",
    "    def steps(self):\n",
    "        \n",
    "        return [ \n",
    "            MRStep(mapper=self.mapper_1, reducer=self.reducer_1),\n",
    "            MRStep(mapper=self.mapper_2, reducer = self.reducer_2),\n",
    "            MRStep(mapper = self.mapper_3, reducer = self.reducer_3),\n",
    "            MRStep(mapper = self.mapper_4)\n",
    "        ]\n",
    "\n",
    "    # Takes a description as input, removes stopwords, punctuation and convert to lowercase\n",
    "    # Returns a list of words.\n",
    "    def split_and_clean(self, line):  \n",
    "        words = []\n",
    "        line = re.sub(special_char, ' ', line) #remove special characters from line\n",
    "        for word in WORD_RE.findall(line): #for each word in the line\n",
    "            if word.lower() not in stopwords and len(word)>1: #if the word is not a stop word and length > 1\n",
    "                words.append(snowball.stem(word.lower())) #append the root of the word to a list\n",
    "        return words #return the list of words\n",
    "\n",
    "    # Mapper of the first MapReduce job\n",
    "    def mapper_1(self, _, line):  \n",
    "        id_found = re.search(r\"(^\\d+),\", line)\n",
    "        description_found = re.search(r\",\\\"(.+)\\\",\", line)\n",
    "        if id_found: \n",
    "            id = int(id_found.group(1))\n",
    "            if description_found:\n",
    "                d = description_found.group(1)\n",
    "                description = str()\n",
    "                flag = True\n",
    "                for char in d:\n",
    "                    if char =='\"':\n",
    "                        flag = False\n",
    "                    if flag:\n",
    "                        description += char\n",
    "                for word in self.split_and_clean(description): \n",
    "                    yield (word,id), 1  \n",
    "\n",
    "\n",
    "    # Reducer of the first MapReduce job\n",
    "    def reducer_1(self, word_id, counts): \n",
    "        yield (word_id[0], word_id[1]), sum(counts) \n",
    "\n",
    "    # Mapper of the second MapReduce job\n",
    "    def mapper_2(self, word_id, n):\n",
    "        word, id = word_id[0], word_id[1], \n",
    "        yield id, (word,n) \n",
    "\n",
    "    # Reducer of the second MapReduce job \n",
    "    def reducer_2(self, id, word_n):\n",
    "        \n",
    "        word_list = []\n",
    "        n_list = []\n",
    "        N = 0 \n",
    "        num_el = 0\n",
    "\n",
    "        for value in word_n:\n",
    "            word, n = value[0], value[1]\n",
    "\n",
    "            word_list.append(word)\n",
    "            n_list.append(n)\n",
    "            N+=int(n)\n",
    "\n",
    "            num_el+=1\n",
    "\n",
    "        for i in range(num_el):\n",
    "            yield (word_list[i],id), (n_list[i],N)   \n",
    "\n",
    "    # Mapper of the third MapReduce job \n",
    "    def mapper_3(self, word_id, n_N):\n",
    "        word, id = word_id[0], word_id[1]\n",
    "        n, N = n_N[0], n_N[1]\n",
    "        yield word, (id, n, N, 1)\n",
    "\n",
    "\n",
    "    # Reducer of the third MapReduce job\n",
    "    def reducer_3(self, word, id_n_N_counts):\n",
    "\n",
    "        id_list = []\n",
    "        n_list = []\n",
    "        N_list = []\n",
    "        m = 0\n",
    "\n",
    "        num_el = 0\n",
    "\n",
    "        for value in id_n_N_counts:\n",
    "            id, n, N, count = value[0], value[1], value[2], value[3]\n",
    "\n",
    "            id_list.append(id)\n",
    "            n_list.append(n)\n",
    "            N_list.append(N)\n",
    "\n",
    "            m+=count\n",
    "\n",
    "            num_el +=1\n",
    "\n",
    "        for i in range(num_el):\n",
    "            yield (word, id_list[i]), (n_list[i], N_list[i],m)\n",
    "\n",
    "    # Mapper of the fourth MapReduce job \n",
    "    def mapper_4(self, word_id, n_N_m):\n",
    "        word, id = word_id[0], word_id[1]\n",
    "        n, N, m = n_N_m[0], n_N_m[1], n_N_m[2]\n",
    "\n",
    "        tf = (n/N) \n",
    "        idf = math.log(M/m) \n",
    "        tfidf = tf*idf\n",
    "\n",
    "        if tfidf > threshold:\n",
    "            yield (word, id), tfidf\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MR_TFIDF.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for inline runner\n",
      "Creating temp directory /tmp/tfidf.sara.20221129.165041.819833\n",
      "Running step 1 of 4...\n",
      "Running step 2 of 4...\n",
      "Running step 3 of 4...\n",
      "Running step 4 of 4...\n",
      "job output is in /tmp/tfidf.sara.20221129.165041.819833/output\n",
      "Streaming final output from /tmp/tfidf.sara.20221129.165041.819833/output...\n",
      "Removing temp directory /tmp/tfidf.sara.20221129.165041.819833...\n"
     ]
    }
   ],
   "source": [
    "! python3 tfidf.py data/books_metadata.csv > tfidf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aae75fb4c4e28f3fda60e5eda70c237ec76be20ea0bc0d2e1a8874fab74ff8f3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
